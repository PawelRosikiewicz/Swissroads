{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. FEATURE EXTRACTION\n",
    "---\n",
    "\n",
    "# README\n",
    "\n",
    "## directories\n",
    "* All files cretaed or used in my noteboks require speciffic directory organization  \n",
    "* it is as follow  \n",
    ">    * basedir   \n",
    "        * __models__ : contains downloaded tf hub model for feature extraction\n",
    "        * __notebooks__ : jupiter noteboks 1-9    \n",
    "        * __src__\n",
    "            * __utils__ : contain .py files require for the analysis, loaded in each notebook, \n",
    "        * __data__\n",
    "            * __raw__\n",
    "                * __swissroads__      : data provided by EPFL-ext (created automatically)\n",
    "            * __interim__ : intermediate and precessed data required for other tasks\n",
    "                * __swissroads__       : extracted features, labels and logfiles (created automatically)\n",
    "            * __results__ :  contains one or more directory for each jopiter notebook : resulst, eg: model predictions, \n",
    "                * __swissroads_cnn__\n",
    "                * __swissroads_knn__\n",
    "                * __swissroads_decision_trees__\n",
    "                * etc...\n",
    "                \n",
    "## basedir:\n",
    "- is is sufficinet to set up basedir in config section in eac h jupyter notebook torepeat the analsis   \n",
    "- in case something is missing please contact __$prosikiewicz@gmail.com$__  \n",
    "\n",
    "## config\n",
    "- to make the control of each task simpler, I added config file to first cell of each notebook, \n",
    "- there are two parts:  \n",
    "    - project config: used by all tasks 1-9  \n",
    "    - task config: speciffic to a given task  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup the enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file for jupyter notebooks was already created\n",
      "file for tfhub model was already created\n",
      "file for config and my tools was already created\n",
      "file IMPORTANT : HERE YOU MUST COPY ALL PY FILES IN UTILS was already created\n",
      "file to store data and resuls was already created\n",
      "file here copy swissroad files with raw images was already created\n",
      "file to store matrices with extracted features was already created\n",
      "file for final results was already created\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    TO START, REPLACE basedir, with path to preffered location in your system\n",
    "    THEN RUN THE COMMAND BELOW, and finally, copy Notebooks to notebook folder, \n",
    "    py file to src/utils, and tfhub to models folder. \n",
    "\"\"\"\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "import os\n",
    "basedir = '/Users/pawel/Desktop/Activities/005__COURSES/000__EPFLext_ADSML/Module 4 __ Project/test'\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "files_to_create = {\n",
    "    \"for jupyter notebooks\": os.path.join(basedir, \"notebooks\"),\n",
    "    \"for tfhub model\": os.path.join(basedir, \"models\"),\n",
    "    \"for config and my tools\": os.path.join(basedir, \"src\"),\n",
    "    \"IMPORTANT : HERE YOU MUST COPY ALL PY FILES IN UTILS\": os.path.join(basedir, \"src/utils\"),\n",
    "    # ....\n",
    "    \"to store data and resuls\": os.path.join(basedir, \"data\"),\n",
    "    \"here copy swissroad files with raw images\": os.path.join(basedir, \"data/raw\"),\n",
    "    \"to store matrices with extracted features\": os.path.join(basedir, \"data/interim\"),\n",
    "    \"for final results\": os.path.join(basedir, \"data/results\")\n",
    "}\n",
    "# ....\n",
    "for file_function in  list(files_to_create.keys()):\n",
    "    try:\n",
    "        os.mkdir(files_to_create[file_function])\n",
    "    except:\n",
    "        print(\"file\", file_function, \"was already created\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************************************************************************* #\n",
    "#     Author:   Pawel Rosikiewicz                                           #       \n",
    "#     Copyrith: IT IS NOT ALLOWED TO COPY OR TO DISTRIBUTE                  #\n",
    "#               these file without written                                  #\n",
    "#               persmission of the Author                                   #\n",
    "#     Contact:  prosikiewicz@gmail.com                                      #\n",
    "#                                                                           #\n",
    "# ************************************************************************* #\n",
    "\n",
    "# Config\n",
    "\n",
    "# basedir\n",
    "basedir = '/Users/pawel/Desktop/Activities/005__COURSES/000__EPFLext_ADSML/Module 4 __ Project/solution'\n",
    "\n",
    "# project config\n",
    "project_name = \"Pawel_Rosikiewicz_EPFLext_Project4\"\n",
    "dataset_name = \"swissroads\"\n",
    "subset_names = [\"train\", \"valid\", \"test\"]\n",
    "module_names = [\"imagenet\"]\n",
    "class_colors = {          \"car\": \"orange\",\n",
    "                          \"van\": \"steelblue\",\n",
    "                          \"truck\": \"red\",\n",
    "                          \"bike\": \"forestgreen\",\n",
    "                          \"motorcycle\" : \"purple\",\n",
    "                           \"other\": \"dimgrey\"}\n",
    "\n",
    "# task config\n",
    "task_name = \"feature_extraction\"\n",
    "subset_names_to_encode = [\"train\", \"valid\", \"test\"]\n",
    "use_url = \"no\" # if yes, module_paths must contain url to tfhub modules, \n",
    "module_paths = [\"imagenet_inception_v3_feature_vector_3\"] # tfmodule names stored in basedir/models\n",
    "img_imput_size = [(299, 299)] # required by imagenet, \n",
    "generator_batch_size = 10000 # nr of images that will be loaded in order of appearance to one batch with enc oded feastures, if less found, all will be used,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # allow changing, and navigating files and folders, \n",
    "import sys\n",
    "import re # module to use regular expressions, \n",
    "import glob # lists names in folders that match Unix shell patterns\n",
    "import random # functions that use and generate random numbers\n",
    "\n",
    "import pickle\n",
    "import warnings\n",
    "import numpy as np # support for multi-dimensional arrays and matrices\n",
    "import pandas as pd # library for data manipulation and analysis\n",
    "import seaborn as sns # advance plots, for statistics, \n",
    "import matplotlib as mpl # to get some basif functions, heping with plot mnaking \n",
    "import matplotlib.pyplot as plt # for making plots, \n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from IPython.display import display\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import scale\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load custom functions created for the project from basedir/src/utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "basedir = os.path.dirname(os.getcwd())\n",
    "os.chdir(basedir)\n",
    "sys.path.append(basedir)\n",
    "\n",
    "# my tools, loaded only form basedir, \n",
    "from src.utils.feature_extraction_tools import encode_images\n",
    "from src.utils.data_loaders import load_encoded_imgbatch_using_logfile, load_raw_img_batch\n",
    "from src.utils.example_plots_after_clustering import plot_img_examples, create_spaces_between_img_clusters, plot_img_examples_from_dendrogram\n",
    "from src.utils.annotated_pie_charts import annotated_pie_chart_with_class_and_group, prepare_img_classname_and_groupname\n",
    "from src.utils.tools_for_plots import create_class_colors_dict\n",
    "from src.utils.logreg_tools import my_logredCV, plot_examples_with_predictions_and_proba, plot_examples_with_predictions_and_proba_gamma\n",
    "from src.utils.random_forest import random_forest_grid_search, plot_random_tree_summary_andor_table\n",
    "from src.utils.model_summary_plots import plot_grid_acc_and_return_summary_df, visual_model_summary\n",
    "from src.utils.SVM_tools import SVM_grid_search, plot_grid_acc_and_return_summary_df\n",
    "from src.utils.tools_for_plots import create_class_colors_dict\n",
    "from src.utils.model_summary_plots import plot_grid_acc_and_return_summary_df, visual_model_summary, model_gridsearch_summary_plots\n",
    "from src.utils.knn_tools import knn_grid_search\n",
    "from src.utils.image_augmentation import *\n",
    "\n",
    "# one of my functions was generating warnign, on copies, despite using proper syntax, \n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# setup paths for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exist, or PATH_results was not created correctly\n"
     ]
    }
   ],
   "source": [
    "PATH_raw = os.path.join(basedir, \"data/raw\", dataset_name)\n",
    "PATH_encoded = os.path.join(basedir, \"data/interim\", dataset_name)\n",
    "PATH_results = os.path.join(basedir, \"data/results\", f\"{dataset_name}_{task_name}\") # not used in that task, but created in case I swoudl like to save metadata in the future, \n",
    "PATH_models = os.path.join(basedir, \"models\")\n",
    "\n",
    "# create new direcory for task results, \n",
    "try: \n",
    "    os.mkdir(PATH_results)\n",
    "    try:\n",
    "        os.chdir(PATH_results)\n",
    "        print(\"PATH_results was created\")\n",
    "    except:\n",
    "        print(\"ERROR: PATH_results was not created correctly\")\n",
    "except: \n",
    "    print(\"file already exist, or PATH_results was not created correctly\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features form images using tfhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ecoding:,imagenet, imagenet_inception_v3_feature_vector_3, (299, 299)\n",
      "Found 280 images belonging to 6 classes.\n",
      "Found 139 images belonging to 6 classes.\n",
      "Found 50 images belonging to 6 classes.\n",
      "\n",
      "\n",
      "................................................................................\n",
      " Creating DataGenerators for: swissroads; imagenet;\n",
      "................................................................................\n",
      "\n",
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n",
      "\n",
      "................................................................................\n",
      " TF Graph;\n",
      "Feature extraction Module, from: /Users/pawel/Desktop/Activities/005__COURSES/000__EPFLext_ADSML/Module 4 __ Project/solution/models/imagenet_inception_v3_feature_vector_3\n",
      "................................................................................\n",
      " Ecoding imgages in one batch for < train > dataset;\n",
      "Feature number = (280, 2048)\n",
      "label table shape = (280, 4)\n",
      "Saved as:\n",
      "imagenet_swissroads_train_encoded.npy and imagenet_swissroads_train_labels.csv\n",
      "saved in:\n",
      "/Users/pawel/Desktop/Activities/005__COURSES/000__EPFLext_ADSML/Module 4 __ Project/solution/data/interim/swissroads\n",
      "................................................................................\n",
      " Ecoding imgages in one batch for < valid > dataset;\n",
      "Feature number = (139, 2048)\n",
      "label table shape = (139, 4)\n",
      "Saved as:\n",
      "imagenet_swissroads_valid_encoded.npy and imagenet_swissroads_valid_labels.csv\n",
      "saved in:\n",
      "/Users/pawel/Desktop/Activities/005__COURSES/000__EPFLext_ADSML/Module 4 __ Project/solution/data/interim/swissroads\n",
      "................................................................................\n",
      " Ecoding imgages in one batch for < test > dataset;\n",
      "Feature number = (50, 2048)\n",
      "label table shape = (50, 4)\n",
      "Saved as:\n",
      "imagenet_swissroads_test_encoded.npy and imagenet_swissroads_test_labels.csv\n",
      "saved in:\n",
      "/Users/pawel/Desktop/Activities/005__COURSES/000__EPFLext_ADSML/Module 4 __ Project/solution/data/interim/swissroads\n",
      "................................................................................\n",
      " Creating logfile for < swissroads >;\n",
      "saved as:  imagenet_swissroads_logfile.csv\n",
      "in: /Users/pawel/Desktop/Activities/005__COURSES/000__EPFLext_ADSML/Module 4 __ Project/solution/data/interim/swissroads\n"
     ]
    }
   ],
   "source": [
    "# encode,  \n",
    "for one_module_name, one_module_path, one_img_input_size in zip(module_names, module_paths, img_imput_size):\n",
    "    '''\n",
    "        all data subsets found in load_dir will be encoded automatically, \n",
    "        - logfile will be created for a given datasets\n",
    "        - batch_labels csv file and npz file with encoded features will be created for \n",
    "        each data subset will have:\n",
    "        - \n",
    "    '''\n",
    "    \n",
    "    print(f\"Ecoding:,{one_module_name}, {one_module_path}, {one_img_input_size}\")\n",
    "    \n",
    "    # I am using modules saved in computer memory, thus I need to build fiull path to them, \n",
    "    if use_url==\"no\":\n",
    "        one_module_full_path = os.path.join(PATH_models, one_module_path)\n",
    "    else:\n",
    "        one_module_full_path = one_module_path\n",
    "    \n",
    "    # extract features    \n",
    "    encode_images(\n",
    "        \n",
    "        # .. dastaset name & directories, \n",
    "        dataset_name=dataset_name,# dataset name used when saving encoded files, logfiles and other things, related to encoding, \n",
    "        subset_names=subset_names_to_encode,# list, ust names of files in the load_dir, if any, \n",
    "        load_dir=PATH_raw,   # full path to input data, ie. file folder with either folders with images names after class names, or folders with subsetnames, and folders names after each class in them, \n",
    "        save_dir=PATH_encoded, # all new files, will be saved as one batch, with logfile, if None, load_dir will be used, \n",
    "\n",
    "        # .. encoding module parameters, \n",
    "        module_name=one_module_name, \n",
    "        module_location=one_module_full_path, # full path to a given module, or url, \n",
    "        img_target_size=one_img_input_size, # image resolution in pixels, \n",
    "        generator_batch_size =generator_batch_size, \n",
    "        generator_shuffle    =False, \n",
    "\n",
    "        # .. other, \n",
    "        save_files=True,\n",
    "        verbose=True                             \n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
